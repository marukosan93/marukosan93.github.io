<!doctype html>
<html>

<head>
  <meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate" />
  <meta charset="utf-8">
  <title>Research</title>
  <link rel="shortcut icon" type="image/gif" href="img/pixel-heart.gif"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport">
  <!--<meta content="some keywords?" name="keywords">-->
  <meta content="EmotionAI project website" name="description">

  <!-- Google Font -->
  <link href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@300;400;600;700;800&display=swap" rel="stylesheet">

  <!-- CSS Libs -->
  <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" rel="stylesheet">
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.10.0/css/all.min.css" rel="stylesheet">
  <link href="lib/animate/animate.min.css" rel="stylesheet">

  <!-- Main stylesheet -->
  <link href="css/style.css" rel="stylesheet">
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-Z9BEGS2P9M"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-Z9BEGS2P9M');
  </script>
</head>

<body>
  <div class="flex-wrapper">
    <!-- Maybe a topbar? But doesn't seem that useful...-->

    <!-- Navbar Start -->
    <div class="navbar navbar-expand-lg bg-dark navbar-dark">
      <div class="container-fluid">
        <a href="index.html" class="navbar-brand">Emotion<span>AI</span></a>
        <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbarCollapse">
          <span class="navbar-toggler-icon"></span>
        </button>

        <div class="collapse navbar-collapse justify-content-between" id="navbarCollapse">
          <div class="navbar-nav ml-auto">
            <a href="index.html" class="nav-item nav-link">Home</a>
            <a href="research.html" class="nav-item nav-link active">Research</a>
            <a href="team.html" class="nav-item nav-link">Team</a>
            <a href="news.html" class="nav-item nav-link">News</a>
            <a href="ads.html" class="nav-item nav-link">Ads</a>
            <a href="contact.html" class="nav-item nav-link">Contact</a>
          </div>
        </div>
      </div>
    </div>
    <!-- Nav Bar End -->

    <!-- Page Header Start -->
    <div class="empty-page-header">

    </div>
    <!-- Page Header End -->

    <!-- About Start -->
    <div class="about wow fadeInUp" data-wow-delay="0.1s">
      <div class="container-fluid">
        <div class="row align-items-center">
          <div class="col-lg-5 col-md-6">
            <div class="about-img wow fadeInUp" data-wow-delay="0.1s">
              <img src="img/main_pic.jpg" alt="Image">
            </div>
          </div>
          <div class="col-lg-7 col-md-6">
            <div class="section-header text-left wow zoomIn" data-wow-delay="0.1s">
              <p>Current project</p>
              <h4>Academy professor project (2021.09-2026.08): <br> Towards vision-based emotion AI (Emotion AI)</h4>

            </div>
            <div class="about-text wow fadeInUp" data-wow-delay="0.1s">
              <p>
                The Emotion AI project aims to deeply investigate novel computer vision and machine learning methodology to study how artificial intelligence (AI) can identify human emotions and bring emotion AI to human-computer interaction and
                computer mediated human-human interaction for boosting remote communication and collaboration. In addition to expressed visual cues, AI technology is expected to identify suppressed and unseen emotional signals, and at the same time
                mask people’s identity information for protecting privacy. We will work with worldwide leading experts from different disciplines, e.g., psychology, cognition sciences, education, and medicine, as well as industry, to advance the
                emotional intelligence of AI-based solutions and improve understanding of the significance of emotions in the context of human-computer interactions. The research knowledge generated can accelerate innovations for e.g., real-world
                e-teaching, e-service, health and security applications. <br>
                <a href="https://www.oulu.fi/blogs/science-with-arctic-attitude/emotion-ai">https://www.oulu.fi/blogs/science-with-arctic-attitude/emotion-ai</a>
              </p>
              <div class="generic-btn">
                <a class="btn wow fadeInUp" data-wow-delay="0.1s" href="research.html#publications">Publications</a>
                <a class="btn wow fadeInUp" data-wow-delay="0.1s" href="research.html#demo">View Demos</a>
                <a class="btn wow fadeInUp" data-wow-delay="0.1s" href="research.html#other">Other Projects</a>
                <a class="btn wow fadeInUp" data-wow-delay="0.1s" href="research.html#data">Data sharing</a>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
    <!-- About End -->

    <!-- wp Start -->
    <div class="wp">
      <div class="container-fluid">
        <div class="section-header text-center wow zoomIn" data-wow-delay="0.1s">
          <p>How EmotionAI is organised</p>
          <h2>Work packages</h2>
        </div>
        <div class="row">
          <div class="col-lg-4 col-md-6 row-eq-height wow fadeInUp" data-wow-delay="0.1s">
            <div class="wp-item">
              <div class="wp-icon">
                <img src="img/wp1.jpeg">
              </div>
              <h3>WP1: Expressed cues (posed or spontaneous facial & bodily expressions)</h3>
              <p>
                Aims: Acquisition of reliable new dataset; Dynamic descriptors; Temporal modeling; Context analysis.
              </p>
              <p>
                People: <b>Hanlin Mo, Qianru Xu</b>
              </p>
            </div>
          </div>
          <div class="col-lg-4 col-md-6 row-eq-height wow fadeInUp" data-wow-delay="0.1s">
            <div class="wp-item">
              <div class="wp-icon">
                <img src="img/wp2.jpg">
              </div>
              <h3>WP2: Suppressed cues (micro expressions & gestures)</h3>
              <p>
                Aims: Acquisition of reliable new dataset; Dynamic descriptors; Temporal modeling; Context analysis; Subtle motion detection and magnification
              </p>
              <p>People: <b>Yante Li, Haoyu Chen</b>
              </p>
            </div>
          </div>
          <div class="col-lg-4 col-md-6 row-eq-height wow fadeInUp" data-wow-delay="0.1s">
            <div class="wp-item">
              <div class="wp-icon">
                <img src="img/wp3.webp">
              </div>
              <h3>WP3: Unseen (physiological signal estimation & analysis)</h3>
              <p>
                Aims: Acquisition of reliable new dataset; Temporal modeling; Context analysis; Subtle motion detection and magnification
              </p>
              <p>
                People:<b> Marko Savic</b>
              </p>
            </div>
          </div>
          <div class="col-lg-4 col-md-6 row-eq-height wow fadeInUp" data-wow-delay="0.1s">
            <div class="wp-item">
              <div class="wp-icon">
                <img src="img/wp4.jpg">
              </div>
              <h3>WP4: Association of different cues</h3>
              <p>
                Aims: Acquisition of reliable new dataset; Context analysis; Multi-model learning
              </p>
              <p>
                People:<b> All</b>
              </p>
            </div>
          </div>
          <div class="col-lg-4 col-md-6 row-eq-height wow fadeInUp" data-wow-delay="0.1s">
            <div class="wp-item">
              <div class="wp-icon">
                <img src="img/wp5.jpeg">
              </div>
              <h3>WP5: Masking identity</h3>
              <p>
                Aims: Acquisition of reliable new dataset; Emotion cue transfer
              </p>
              <p>
                People: <b>Kevin Ho Man Cheng, Marko Savic, Haoyu Chen</b>
              </p>
            </div>
          </div>
          <div class="col-lg-4 col-md-6 row-eq-height wow fadeInUp" data-wow-delay="0.1s">
            <div class="wp-item">
              <div class="wp-icon">
                <img src="img/wp6.jpg">
              </div>
              <h3>WP6: Experimental validation</h3>
              <p>
                Aims: Acquisition of reliable new dataset; Context analysis; Multi-model learning; Emotion cue transfer
              </p>
              <p>
                People: <b>All</b>
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
    <!-- wp End -->

    <!-- publications start -->
    <div class="publications wow fadeInUp" data-wow-delay="0.1s">
      <div class="container-fluid" id="publications">
        <div class="section-header text-center wow zoomIn" data-wow-delay="0.1s">
          <p> Project </p>
          <h4>Publications</h4>
        </div>

        <p class="wow fadeInUp" data-wow-delay="0.1s">You can also find all the other publications from our unit by going to our PI's <a href="https://gyzhao-nm.github.io/Guoying/">personal site</a> or her <a
            href="https://www.oulu.fi/university/researcher/guoying-zhao">UniOulu page</a></p>

        <table class="table  table-hover wow fadeInUp" data-wow-delay="0.1s">
          <thead>
            <tr>
              <th class="w-75" scope="col">Article</th>
              <th class="w-25 text-right align-self-center" scope="col">Year</th>
            </tr>
          </thead>
          <tbody>

            <!-- paper -->
            <tr class="wow fadeInUp" data-wow-delay="0.1s">
              <td class="w-75">
                <a href="https://www.sciltp.com/journals/ijndi/article/view/115">From Emotion AI to Cognitive AI</a>
                <p>G Zhao, Y Li , and Q Xu</p>
                <p>International Journal of Network Dynamics and Intelligence (IJNDI)</p>
              </td>
              <th class="w-25 text-right" scope="row">2022</th>
            </tr>
            <!-- paper -->
            <!-- paper -->
            <tr class="wow fadeInUp" data-wow-delay="0.1s">
              <td class="w-75">
                <a href="https://ieeexplore.ieee.org/document/9897232">Benchmarking 3D Face De-identification with Preserving Facial Attributes</a>
                <p>K H M Cheng, Z Yu, H Chen, G Zhao</p>
                <p>IEEE International Conference on Image Processing, ICIP 2022</p>
              </td>
              <th class="w-25 text-right" scope="row">2022</th>
            </tr>
            <!-- paper -->
            <tr class="wow fadeInUp" data-wow-delay="0.1s">
              <td class="w-75">
                <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9915437">Deep Learning for Micro-expression Recognition: A Survey</a>
                <p>Y Li, J Wei, Y Liu, J Kauttonen, G Zhao</p>
                <p>IEEE Transactions on Affective Computing, Vol. 13, NO. 4</p>
              </td>
              <th class="w-25 text-right" scope="row">2022</th>
            </tr>
            <!-- paper -->
            <tr class="wow fadeInUp" data-wow-delay="0.1s">
              <td class="w-75">
                <a href="https://arxiv.org/abs/2112.07374">Geometry-Contrastive Transformer for Generalized 3D Pose Transfer</a>
                <p>H Chen, H Tang, Z Yu,N Sebe, G Zhao</p>
                <p>Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2022</p>
              </td>
              <th class="w-25 text-right" scope="row">2022</th>
            </tr>
            <!-- paper -->
            <tr class="wow fadeInUp" data-wow-delay="0.1s">
              <td class="w-75">
                <a href="https://arxiv.org/abs/2110.10533">AniFormer: Data-driven 3D Animation with Transformer</a>
                <p>H Chen, H Tang, N Sebe, G Zhao</p>
                <p>Proceedings of the British Machine Vision Conference, 2021</p>
                <a href="https://github.com/mikecheninoulu/AniFormer">https://github.com/mikecheninoulu/AniFormer</a>
              </td>
              <th class="w-25 text-right" scope="row">2021</th>
            </tr>


            <!-- Newer papers should be on top -->

          </tbody>
        </table>

      </div>

    </div>
  </div>

  <!-- publications end -->


  <!-- Demo start -->
  <div class="demo wow fadeInUp" data-wow-delay="0.1s">
    <div class="container-fluid" id="demo">
      <div class="section-header text-center wow zoomIn" data-wow-delay="0.1s">
        <p>Our research in action</p>
        <h4>Demos</h4>
      </div>
      <h5>Geometry-Contrastive Transformer for Generalized 3D Pose Transfer</h5>
      <p> </p>
      <div class="video wow fadeInUp" data-wow-delay="0.1s">
        <!--<iframe width="420" height="345" src="https://www.youtube.com/embed/WStoojF1VHA"></iframe>-->
        <!--<img src="img/coming_soon.png" width="40%">-->
        <video width="80%" controls>
         <source src="video/demo_GCT.mp4" type="video/mp4">
        Your browser does not support the video tag.
        </video>
      </div>
    </div>
  </div>
  <!-- Demo end -->


  <!-- Other  start-->
  <div class="other wow fadeInUp" data-wow-delay="0.1s">
    <div class="container-fluid" id="other">
      <div class="section-header text-center wow zoomIn" data-wow-delay="0.1s">
        <p> Other</p>
        <h4>Projects</h4>
      </div>
      <p class="wow fadeInUp" data-wow-delay="0.1s">There are several other projects we are working on, such as:</p>
      <div class="row justify-content-center">
        <div class="col-xxl-6 col-xl-6 col-lg-6 col-md-12  col-sm-12  wow fadeInUp" data-wow-delay="0.1s">
          <div class="other-item">
            <h3>MiGA project</h3>
            <p>Academy project (2018.09-2022.08): Micro-gesture analysis with machine learning for hidden emotion understanding (MiGA)
              MiGA project aims to understand human hidden emotions via body gestures. Micro-gestures are inconspicuous, spontaneous gestures, most of which are beyond our awareness, or unconscious. Being able to automatically detect and then amplify
              such gestures so that enables one to discover their symbolic meaning, opening up rich paths of emotional intelligence. In this project, we introduced a new dataset for the emotional artificial intelligence research: identity-free video
              dataset for Micro Gesture Understanding and Emotion analysis (iMiGUE), which contains 359 short-length videos of 72 famous tennis players. This project is proposed for purely research purposes to enhance the algorithms of recognition of
              micro-gestures and emotions.<br>
            </p>
          </div>
        </div>
        <div class="col-xxl-6 col-xl-6 col-lg-6 col-md-12  col-sm-12  wow fadeInUp" data-wow-delay="0.1s">

          <div class="column justify-content-center">
            <div class="row-xxl-6 row-xl-6 row-lg-6 row-md-12  row-sm-12  wow fadeInUp" data-wow-delay="0.1s">
              <div class="other-item">
                <h3>Academy ICT project</h3>
                <p>
                  Academy ICT 2023 project (2020.01-2022.12): Context-aware autonomous neural network learning <br>
                </p>
              </div>
            </div>

            <div class="row-xxl-6 row-xl-6 row-lg-6 row-md-12  row-sm-12  wow fadeInUp" data-wow-delay="0.1s">
              <div class="other-item">
                <h3>Spearhead project</h3>
                <p>
                  Spearhead project (2018.09-2022.08): Towards Reading Micro-Expressions in the Real World

                  <br>
                </p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>




  <!-- Data  start-->
  <div class="data wow fadeInUp" data-wow-delay="0.1s">
    <div class="container-fluid" id="data">
      <div class="section-header text-center wow zoomIn" data-wow-delay="0.1s">
        <p> Sharing</p>
        <h4>Datasets</h4>
      </div>
      <p class="wow fadeInUp" data-wow-delay="0.1s">There are several datasets we share with the research community, which are briefly introduced below. If you are interested, please contact us.</p>
      <div class="row justify-content-center">
        <div class="col-xxl-4 col-xl-6 col-lg-6 col-md-12  col-sm-12  wow fadeInUp" data-wow-delay="0.1s">
          <div class="data-item">
            <h3>OuluVS database</h3>
            <p>
              Oulu VS database includes the video and audio data for 20 subjects uttering ten phrases: Hello, Excuse me, I am sorry, Thank you, Good bye, See you, Nice to meet you, You are welcome, How are you, Have a good time. Each person spoke
              each phrase five times. There are also videos with head motion from front to left, from front to right, without utterance, five times for each person. The database can be used, for example, in studying the visual speech recognition
              (lipreading).
              <br>
              <br>
              The details and the baseline results can be found in:
              <a href="https://ieeexplore.ieee.org/document/5208233">Zhao G, Barnard M & Pietikäinen M (2009). Lipreading with local spatiotemporal descriptors. IEEE Transactions on Multimedia 11(7):1254-1265</a>
            </p>
          </div>
        </div>
        <div class="col-xxl-4 col-xl-6 col-lg-6 col-md-12  col-sm-12  wow fadeInUp" data-wow-delay="0.1s">
          <div class="data-item">
            <h3>OuluVS2 database</h3>
            <p>
              Oulu VS2 is a multi-view audiovisual database for non-rigid mouth motion analysis. It includes more than 50 speakers uttering three types of utterances and more importantly, thousands of videos simultaneously recorded by six cameras from ﬁve different views spanned between the frontal and proﬁle views.
              <br>
              <br>
              The details and the baseline results can be found in:
              <a href="https://ieeexplore.ieee.org/document/7163155">Anina, I., Zhou, Z., Zhao, G., & Pietikäinen, M. (2015). OuluVS2: A multi-view audiovisual database for non-rigid mouth motion analysis. In 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG) (pp. 1-5).</a>
            </p>
          </div>
        </div>
        <div class="col-xxl-4 col-xl-6 col-lg-6 col-md-12  col-sm-12   wow fadeInUp" data-wow-delay="0.1s">
          <div class="data-item">
            <h3>Oulu-CASIA NIR&VIS facial expression database</h3>
            <p>
              Oulu-CASIA NIR&VIS facial expression database contains videos with the six typical expressions (happiness, sadness, surprise,anger, fear, disgust) from 80 subjects captured with two imaging systems, NIR (Near Infrared) and VIS (Visible
              light), under three different illumination conditions: normal indoor illumination, weak illumination (only computer display is on) and dark illumination (all lights are off). The database can be used, for example, in studying the
              effects of illumination variations to facial expressions, cross-imaging-system facial expression recognition or face recognition.
              <br>
              <br>
              The details and the baseline results can be found in:
              <a href="https://ieeexplore.ieee.org/abstract/document/4761697">Zhao, G., Huang, X., Taini, M., Li, S. Z., & PietikäInen, M. (2011). Facial expression recognition from near-infrared videos. Image and Vision Computing, 29(9),
                607-619.</a>
            </p>
          </div>
        </div>
        <div class="col-xxl-4 col-xl-6 col-lg-6 col-md-12  col-sm-12  wow fadeInUp" data-wow-delay="0.1s">
          <div class="data-item">
            <h3>SPOS database - spontaneous and posed facial expressions database</h3>
            <p>
              SPOS database includes spontaneous and posed facial expressions of 7 subjects. Emotional movie clips were shown to subjects to induce spontaneous facial expressions, which include six categories of basic emotions (happy, sad, anger,
              surprise, fear disgust). Subjects were also asked to pose the six kinds of facial expressions after watching movie clips. Data are recorded by both visual and near infer-red camera. All together 84 posed and 147 spontaneous facial
              expression clips were labeled out from the starting point to the apex.
              <br>
              So far, spontaneous and posed facial expressions are usually found in different databases. The difference between databases (different experimental setting and different participants) hindered researches which considered both
              spontaneous and posed facial expressions. This database offers data collected on the same participants and under the same recording condition, which can be used for comparing or distinguishing spontaneous and posed facial expressions
              <br>
              <br>
              The details and the baseline results can be found in:
              <a href="https://ieeexplore.ieee.org/abstract/document/6126401">Pfister, T.; Xiaobai Li; Guoying Zhao; Pietikainen, M., "Recognising spontaneous facial micro-expressions," in Computer Vision (ICCV), 2011 IEEE International Conference on
                , vol., no., pp.1449-1456, 6-13 Nov. 2011</a>
            </p>
          </div>
        </div>
        <div class="col-xxl-4 col-xl-6 col-lg-6 col-md-12  col-sm-12  wow fadeInUp" data-wow-delay="0.1s">
          <div class="data-item">
            <h3>SMIC database - spontaneous micro-expression database</h3>
            <p>
            Micro-expressions are important clues for analyzing people's deceitful behaviors. So far the lack of training source has been hindering research about automatic micro-expression recognition, and SMIC was developed to fill this gap. SMIC database includes spontaneous micro-expressions elicited by emotional movie clips. Emotional movie clips that can induce strong emotion reactions were shown to subjects, and subjects were asked to HIDE their true feelings while watching movie clips. If they failed to do so they will have to fill in a long boring questionnaire as punishment. This kind of setting is to create a high-stake lie situation so that micro-expressions can be induced.
              <br>
              There are also extended versions of SMIC (referred as SMIC-E and SMIC-E-Long), which include long video clips that contains some extra non-micro frames before and after the labelled out micro frames and a lot more contextual frames containing various spontaneous movements.
              <br>
              <br>
              The details can be found in the the following publications:
              <a href="https://ieeexplore.ieee.org/abstract/document/6126401">SMIC-sub</a>,
              <a href="https://tomas.pfister.fi/files/li2013microexpressions.pdf">SMIC</a>,
              <a href="http://jultika.oulu.fi/files/nbnfi-fe2019060618851.pdf">SMIC-E</a>,
              <a href="https://www.sciencedirect.com/science/article/pii/S092523122100268X">SMIC-E-Long</a>
            </p>
          </div>
        </div>

      </div>


    </div>

    <!-- Footer -->
  </div>
  <footer class="page-footer font-small black  " >

    <!-- Copyright -->
    <div class="footer-copyright text-center py-3 wow fadeInUp" data-wow-delay="0.1s">© 2021 Copyright</div>
    <!-- Copyright -->

  </footer>
  <!-- Footer -->


  <!-- JavaScript Libs -->
  <script src="https://code.jquery.com/jquery-3.4.1.min.js"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.bundle.min.js"></script>
  <script src="lib/wow/wow.min.js"></script>

  <!-- Main Javascript -->
  <script src="js/main.js"></script>
</body>

</html>
